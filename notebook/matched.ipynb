{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "555d8376",
   "metadata": {},
   "source": [
    "# Matched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5280c368",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d60ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Automatically detect the repo root (parent of notebook folder)\n",
    "repo_root = Path().resolve().parent  # if notebook is in 'notebooks/' folder\n",
    "sys.path.append(str(repo_root))\n",
    "\n",
    "from config.config import get_environment\n",
    "\n",
    "from config.config import data_import_json, data_import_pandas, data_export_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcca213",
   "metadata": {},
   "source": [
    "## ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ff1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = get_environment(\n",
    "    env_path=\"../environments\",\n",
    "    env_name=\"env.json\"\n",
    ")\n",
    "\n",
    "# content_date = datetime.now().date() + timedelta(days=0)\n",
    "content_date = ENV['CONTENT_DATE']\n",
    "version = ENV['VERSION']\n",
    "\n",
    "website = 'all'\n",
    "source = ENV['SOURCE']['NAME']\n",
    "target1 = ENV['TARGET'][\"1\"]['NAME']\n",
    "target2 = ENV['TARGET'][\"2\"]['NAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c16da7",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92335c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = data_import_pandas(\n",
    "    website=website,\n",
    "    content_date=content_date,\n",
    "    version=version,\n",
    "    folder_name=f'normalize/{website}',\n",
    "    additional_info='normalize'\n",
    ")\n",
    "\n",
    "# Split source target\n",
    "df_source = df_input[df_input['website'] == source].copy(deep=True).reset_index(drop=True)\n",
    "df_target1 = df_input[df_input['website'] == target1].copy(deep=True).reset_index(drop=True)\n",
    "df_target2 = df_input[df_input['website'] == target2].copy(deep=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455bdcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match_matched1 = data_import_pandas(\n",
    "    website=f'{source}_{target1}',\n",
    "    content_date=content_date,\n",
    "    version=version,\n",
    "    folder_name=f'matching/{website}',\n",
    "    additional_info='matching_matched'\n",
    ")\n",
    "\n",
    "df_match_matched1[f'{source}_item_id'] = df_match_matched1[f'{source}_{target1}_id'].str.split('---').str[0]\n",
    "df_match_matched1[f'{target1}_item_id'] = df_match_matched1[f'{source}_{target1}_id'].str.split('---').str[1]\n",
    "\n",
    "df_match_matched2 = data_import_pandas(\n",
    "    website=f'{source}_{target2}',\n",
    "    content_date=content_date,\n",
    "    version=version,\n",
    "    folder_name=f'matching/{website}',\n",
    "    additional_info='matching_matched'\n",
    ")\n",
    "\n",
    "df_match_matched2[f'{source}_item_id'] = df_match_matched2[f'{source}_{target2}_id'].str.split('---').str[0]\n",
    "df_match_matched2[f'{target2}_item_id'] = df_match_matched2[f'{source}_{target2}_id'].str.split('---').str[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da28b6e8",
   "metadata": {},
   "source": [
    "## Overview Matched Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1474007",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_list = list()\n",
    "\n",
    "data_dict1 = {\n",
    "    'pair': f'{source} - {target1}',\n",
    "    '#_source_input': len(df_source),\n",
    "    '#_source_matched': len(df_match_matched1[f'{source}_item_id'].unique()),\n",
    "    '%_source_matched': round(len(df_match_matched1[f'{source}_item_id'].unique())*100/len(df_source), 2),\n",
    "    '#_target_input': len(df_target1),\n",
    "    '#_target_matched': len(df_match_matched1[f'{target1}_item_id'].unique()),\n",
    "    '%_target_matched': round(len(df_match_matched1[f'{target1}_item_id'].unique())*100/len(df_target1), 2),\n",
    "    \n",
    "}\n",
    "\n",
    "count_list.append(data_dict1)\n",
    "\n",
    "data_dict2 = {\n",
    "    'pair': f'{source} - {target2}',\n",
    "    '#_source_input': len(df_source),\n",
    "    '#_source_matched': len(df_match_matched2[f'{source}_item_id'].unique()),\n",
    "    '%_source_matched': round(len(df_match_matched2[f'{source}_item_id'].unique())*100/len(df_source), 2),\n",
    "    '#_target_input': len(df_target2),\n",
    "    '#_target_matched': len(df_match_matched2[f'{target2}_item_id'].unique()),\n",
    "    '%_target_matched': round(len(df_match_matched2[f'{target2}_item_id'].unique())*100/len(df_target2), 2),\n",
    "    \n",
    "}\n",
    "\n",
    "count_list.append(data_dict2)\n",
    "\n",
    "df_count = pd.DataFrame(count_list)\n",
    "\n",
    "data_export_pandas(\n",
    "    df_output=df_count,\n",
    "    website=website,\n",
    "    content_date=content_date,\n",
    "    version=version,\n",
    "    folder_name=f'matched/{website}',\n",
    "    additional_info='matched_count',\n",
    "    file_extension='xlsx'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4995644",
   "metadata": {},
   "source": [
    "## Merge Across Websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c4af7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns source with suffix\n",
    "exclude_cols = []\n",
    "rename_cols = [col for col in df_source.columns if col not in exclude_cols]\n",
    "df_source.rename(columns={\n",
    "    col: f\"{source}_{col}\" for col in rename_cols\n",
    "}, inplace=True)\n",
    "\n",
    "# Rename columns target with suffix\n",
    "exclude_cols = []\n",
    "rename_cols = [col for col in df_target1.columns if col not in exclude_cols]\n",
    "df_target1.rename(columns={\n",
    "    col: f\"{target1}_{col}\" for col in rename_cols\n",
    "}, inplace=True)\n",
    "\n",
    "# Rename columns target with suffix\n",
    "exclude_cols = []\n",
    "rename_cols = [col for col in df_target2.columns if col not in exclude_cols]\n",
    "df_target2.rename(columns={\n",
    "    col: f\"{target2}_{col}\" for col in rename_cols\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff849d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge target id per website\n",
    "df_matched = df_source.copy(deep=True)\n",
    "df_matched[f'{target1}_item_id'] = df_matched[f'{source}_item_id'].map(\n",
    "    df_match_matched1.set_index(f'{source}_item_id')[f'{target1}_item_id'].to_dict()\n",
    ")\n",
    "df_matched[f'{target2}_item_id'] = df_matched[f'{source}_item_id'].map(\n",
    "    df_match_matched2.set_index(f'{source}_item_id')[f'{target2}_item_id'].to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge first target\n",
    "df_matched = pd.merge(\n",
    "    left=df_matched,\n",
    "    right=df_target1[[f'{target1}_item_id', f'{target1}_brand', f'{target1}_item_name', f'{target1}_item_url', f'{target1}_in_stock', f'{target1}_review_total', f'{target1}_review_rating', f'{target1}_price', f'{target1}_price_after_disc', f'{target1}_price_disc']],\n",
    "    on=f'{target1}_item_id',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c6f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge second target\n",
    "df_matched = pd.merge(\n",
    "    left=df_matched,\n",
    "    right=df_target2[[f'{target2}_item_id', f'{target2}_brand', f'{target2}_item_name', f'{target2}_item_url', f'{target2}_in_stock', f'{target2}_review_total', f'{target2}_review_rating', f'{target2}_price', f'{target2}_price_after_disc', f'{target2}_price_disc']],\n",
    "    on=f'{target2}_item_id',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37169c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create match flag count\n",
    "df_matched['match'] = np.select(\n",
    "    [\n",
    "        df_matched[[f'{target1}_item_id', f'{target2}_item_id']].notna().all(axis=1),\n",
    "        df_matched[[f'{target1}_item_id', f'{target2}_item_id']].notna().any(axis=1),\n",
    "    ],\n",
    "    [\n",
    "        2,\n",
    "        1\n",
    "    ],\n",
    "    default=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf2cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Necessary columns only\n",
    "static_cols1 = [f'{source}_scrape_date', 'match']\n",
    "\n",
    "dynamic_cols2 = ['item_id']\n",
    "dynamic_cols2 = [f'{prefix}_{col}' for col in dynamic_cols2 for prefix in [source, target1, target2]]\n",
    "\n",
    "static_cols3 = [f'{source}_category', f'{source}_is_package']\n",
    "\n",
    "dynamic_cols4 = ['brand', 'item_name']\n",
    "dynamic_cols4 = [f'{prefix}_{col}' for col in dynamic_cols4 for prefix in [source, target1, target2]]\n",
    "\n",
    "static_cols5 = [f'{source}_currency']\n",
    "\n",
    "dynamic_cols6 = ['price', 'price_after_disc', 'price_disc', 'in_stock', 'review_total', 'review_rating', 'item_url']\n",
    "dynamic_cols6 = [f'{prefix}_{col}' for col in dynamic_cols6 for prefix in [source, target1, target2]]\n",
    "\n",
    "df_matched = df_matched[static_cols1 + dynamic_cols2 + static_cols3 + dynamic_cols4 + static_cols5 + dynamic_cols6].copy(deep=True)\n",
    "\n",
    "# Rename columns\n",
    "df_matched.rename(columns={\n",
    "    f'{source}_scrape_date': 'scrape_date',\n",
    "    f'{source}_category': 'category',\n",
    "    f'{source}_is_package': 'is_package',\n",
    "    f'{source}_currency': 'currency',\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f877408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_export_pandas(\n",
    "    df_output=df_matched,\n",
    "    website=website,\n",
    "    content_date=content_date,\n",
    "    version=version,\n",
    "    folder_name=f'matched/{website}',\n",
    "    additional_info='matched_all',\n",
    "    incl_excel=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0259bf10",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
