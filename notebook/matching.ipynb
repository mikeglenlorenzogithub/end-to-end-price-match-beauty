{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0f0321",
   "metadata": {},
   "source": [
    "# Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f68adc8",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d0b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Automatically detect the repo root (parent of notebook folder)\n",
    "repo_root = Path().resolve().parent  # if notebook is in 'notebooks/' folder\n",
    "sys.path.append(str(repo_root))\n",
    "\n",
    "from config.config import get_environment\n",
    "\n",
    "from config.config import data_import_json, data_import_pandas, data_export_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c5bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import recordlinkage as rl\n",
    "from recordlinkage.preprocessing import clean\n",
    "from recordlinkage.compare import String#, CompareFunction\n",
    "\n",
    "from rapidfuzz import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2746f0a7",
   "metadata": {},
   "source": [
    "## ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de513e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = get_environment(\n",
    "    env_path=\"../environments\",\n",
    "    env_name=\"env.json\"\n",
    ")\n",
    "\n",
    "# content_date = datetime.now().date() + timedelta(days=0)\n",
    "content_date = ENV['CONTENT_DATE']\n",
    "version = ENV['VERSION']\n",
    "\n",
    "website = 'all'\n",
    "source = ENV['SOURCE']['NAME']\n",
    "# target = ENV['TARGET'][\"1\"]['NAME']\n",
    "target = ENV['TARGET'][\"2\"]['NAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ef315a",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d49d35",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847fefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = data_import_pandas(\n",
    "    website=website,\n",
    "    content_date=content_date,\n",
    "    version=version,\n",
    "    folder_name=f'normalize/{website}',\n",
    "    additional_info='normalize'\n",
    ")\n",
    "\n",
    "# Split source target\n",
    "df_source = df_input[df_input['website'] == source].copy(deep=True).reset_index(drop=True)\n",
    "df_target = df_input[df_input['website'] == target].copy(deep=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6157d1",
   "metadata": {},
   "source": [
    "### Process Matching 1st Brand (Simple Computational Waste) - BACKUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc3a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKUP\n",
    "# # --- Preprocessing (Cleaning) ---\n",
    "# df_source[\"clean_brand\"] = clean(df_source[\"clean_brand\"])\n",
    "# df_target[\"clean_brand\"] = clean(df_target[\"clean_brand\"])\n",
    "\n",
    "# df_source[\"clean_name\"] = clean(df_source[\"clean_name\"])\n",
    "# df_target[\"clean_name\"] = clean(df_target[\"clean_name\"])\n",
    "\n",
    "# threshold_brand = 0.6\n",
    "# indexer = rl.Index()\n",
    "# indexer.full() #.block('brand')\n",
    "\n",
    "# pairs = indexer.index(df_source, df_target)\n",
    "\n",
    "\n",
    "# # --- First-level comparer ---\n",
    "# comparer = rl.Compare()\n",
    "# comparer.add(\n",
    "#     String(\n",
    "#         \"clean_brand\",\n",
    "#         \"clean_brand\",\n",
    "#         method='cosine',   # broad filter\n",
    "#         label=\"score_brand\"\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Enable progress bar\n",
    "# comparer.progress = True\n",
    "\n",
    "# compare_result = comparer.compute(pairs, df_source, df_target)\n",
    "\n",
    "# # flag = 1 if score â‰¥ threshold\n",
    "# compare_result[\"flag_brand\"] = (compare_result[\"score_brand\"] >= threshold_brand).astype(int)\n",
    "\n",
    "# # Filter to reduce computational process\n",
    "# compare_result = compare_result[compare_result['score_brand'] > threshold_brand/2].reset_index(drop=False)\n",
    "\n",
    "# compare_result.rename(columns={\n",
    "#     'level_0': 'index_source',\n",
    "#     'level_1': 'index_target'\n",
    "# }, inplace=True)\n",
    "\n",
    "# # Create pivot merge\n",
    "# df_source_match = df_source.copy(deep=True)\n",
    "# df_target_match = df_target.copy(deep=True)\n",
    "\n",
    "# df_source_match['index_source'] = df_source_match.index\n",
    "# df_target_match['index_target'] = df_target_match.index\n",
    "\n",
    "# # Rename columns source with suffix\n",
    "# exclude_cols = ['index_source']\n",
    "# rename_cols = [col for col in df_source_match.columns if col not in exclude_cols]\n",
    "# df_source_match.rename(columns={\n",
    "#     col: f\"{source}_{col}\" for col in rename_cols\n",
    "# }, inplace=True)\n",
    "\n",
    "# # Rename columns target with suffix\n",
    "# exclude_cols = ['index_target']\n",
    "# rename_cols = [col for col in df_target_match.columns if col not in exclude_cols]\n",
    "# df_target_match.rename(columns={\n",
    "#     col: f\"{target}_{col}\" for col in rename_cols\n",
    "# }, inplace=True)\n",
    "\n",
    "# # Merge comparison result to the initial data\n",
    "# df_match = pd.merge(\n",
    "#     left=df_source_match,\n",
    "#     right=compare_result,\n",
    "#     on='index_source',\n",
    "#     how='left'\n",
    "# )\n",
    "\n",
    "# # Merge safely (deduplicates target index)\n",
    "# df_match = pd.merge(\n",
    "#     left=df_match[df_match['index_target'].notna()],\n",
    "#     right=df_target_match,\n",
    "#     on='index_target',\n",
    "#     how='left'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b7801e",
   "metadata": {},
   "source": [
    "### Process Matching 1st Brand (Complex Computational Saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a7796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "threshold_brand = 0.6  # similarity threshold\n",
    "score_brand_col = f'{source}_{target}_score_brand'\n",
    "flag_brand_col = f'{source}_{target}_flag_brand'\n",
    "source_brand_col = f'{source}_clean_brand'\n",
    "target_brand_col = f'{target}_clean_brand'\n",
    "source_brand_index = f'{source}_index_brand'\n",
    "target_brand_index = f'{target}_index_brand'\n",
    "\n",
    "# Preprocessing (Cleaning)\n",
    "df_source['clean_brand'] = clean(df_source['clean_brand'].fillna(''))\n",
    "df_target['clean_brand'] = clean(df_target['clean_brand'].fillna(''))\n",
    "\n",
    "# Create unique brand data\n",
    "df_source_brand = df_source[['clean_brand']].drop_duplicates().reset_index(drop=True)\n",
    "df_target_brand = df_target[['clean_brand']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Indexing\n",
    "indexer = rl.Index()\n",
    "indexer.full()  # For exact blocking, use indexer.block('clean_brand') if many exact matches\n",
    "brand_pairs = indexer.index(df_source_brand, df_target_brand)\n",
    "\n",
    "# Compare brands\n",
    "comparer = rl.Compare()\n",
    "comparer.add(\n",
    "    String(\n",
    "        'clean_brand',\n",
    "        'clean_brand',\n",
    "        method='cosine', # ['jaro', 'jarowinkler', 'levenshtein', 'damerau_levenshtein', 'qgram', 'cosine', 'smith_waterman', 'lcs']\n",
    "        label=score_brand_col\n",
    "    )\n",
    ")\n",
    "comparer.progress = True  # enable progress bar\n",
    "\n",
    "compare_result_brand = comparer.compute(brand_pairs, df_source_brand, df_target_brand)\n",
    "\n",
    "# Add flag based on threshold\n",
    "compare_result_brand[flag_brand_col] = (compare_result_brand[score_brand_col] >= threshold_brand).astype(int)\n",
    "\n",
    "# Map indices back to brand names\n",
    "compare_result_brand = compare_result_brand.reset_index(drop=False)\n",
    "compare_result_brand.rename(columns={\n",
    "    'level_0': source_brand_index,\n",
    "    'level_1': target_brand_index\n",
    "}, inplace=True)\n",
    "\n",
    "compare_result_brand[source_brand_col] = df_source_brand.loc[compare_result_brand[source_brand_index], 'clean_brand'].values\n",
    "compare_result_brand[target_brand_col] = df_target_brand.loc[compare_result_brand[target_brand_index], 'clean_brand'].values\n",
    "\n",
    "data_export_pandas(\n",
    "    df_output=compare_result_brand,\n",
    "    website=f'{source}_{target}',\n",
    "    content_date=content_date,\n",
    "    version=version,\n",
    "    folder_name=f'matching/{website}',\n",
    "    additional_info='matching_brand_unique',\n",
    "    file_extension='xlsx'\n",
    ")\n",
    "\n",
    "# Pivot using the score\n",
    "df_pivot_brand = compare_result_brand.pivot_table(\n",
    "    index=source_brand_col,\n",
    "    columns=target_brand_col,\n",
    "    values=score_brand_col,\n",
    "    fill_value=0  # fill missing comparisons with 0\n",
    ")\n",
    "\n",
    "# Flatten columns\n",
    "df_pivot_brand = df_pivot_brand.reset_index().rename_axis(None, axis=1)\n",
    "\n",
    "# Add top_score and top_match\n",
    "# top_score\n",
    "df_pivot_brand.insert(1, 'top_score', df_pivot_brand.drop(columns=[source_brand_col]).max(axis=1))\n",
    "\n",
    "# top_match\n",
    "df_pivot_brand.insert(1, 'top_match', df_pivot_brand.drop(columns=[source_brand_col, 'top_score']).idxmax(axis=1))\n",
    "\n",
    "data_export_pandas(\n",
    "    df_output=df_pivot_brand,\n",
    "    website=f'{source}_{target}',\n",
    "    content_date=content_date,\n",
    "    version=version,\n",
    "    folder_name=f'matching/{website}',\n",
    "    additional_info='matching_brand_pivot',\n",
    "    file_extension='xlsx'\n",
    ")\n",
    "\n",
    "# Filter only score > half of threshold\n",
    "compare_result_brand = compare_result_brand[compare_result_brand[score_brand_col] > threshold_brand/2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46605c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge back to full dataset\n",
    "df_source_match = df_source.merge(\n",
    "    compare_result_brand[[source_brand_index, target_brand_index, source_brand_col, target_brand_col, score_brand_col, flag_brand_col]],\n",
    "    left_on='clean_brand',\n",
    "    right_on=source_brand_col,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_target_match = df_target.merge(\n",
    "    compare_result_brand[~compare_result_brand[target_brand_index].duplicated()][[source_brand_index, target_brand_index, source_brand_col, target_brand_col, score_brand_col, flag_brand_col]],\n",
    "    left_on='clean_brand',\n",
    "    right_on=target_brand_col,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop helper columns if desired\n",
    "df_source_match.drop(columns=[source_brand_col, target_brand_col], inplace=True)\n",
    "df_target_match.drop(columns=[source_brand_index, source_brand_col, target_brand_col, score_brand_col, flag_brand_col], inplace=True)\n",
    "\n",
    "# Rename columns source with suffix\n",
    "exclude_cols = [source_brand_index, target_brand_index, source_brand_col, target_brand_col, score_brand_col, flag_brand_col]\n",
    "rename_cols = [col for col in df_source_match.columns if col not in exclude_cols]\n",
    "df_source_match.rename(columns={\n",
    "    col: f\"{source}_{col}\" for col in rename_cols\n",
    "}, inplace=True)\n",
    "\n",
    "# Rename columns target with suffix\n",
    "exclude_cols = [source_brand_index, target_brand_index, source_brand_col, target_brand_col, score_brand_col, flag_brand_col]\n",
    "rename_cols = [col for col in df_target_match.columns if col not in exclude_cols]\n",
    "df_target_match.rename(columns={\n",
    "    col: f\"{target}_{col}\" for col in rename_cols\n",
    "}, inplace=True)\n",
    "\n",
    "# Filter empty source_brand_index to avoid unnecessary empty merge explode\n",
    "df_source_match = df_source_match[df_source_match[target_brand_index].notna()]\n",
    "df_target_match = df_target_match[df_target_match[target_brand_index].notna()]\n",
    "\n",
    "df_match = pd.merge(\n",
    "    left=df_source_match,\n",
    "    right=df_target_match,\n",
    "    on=target_brand_index,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "\n",
    "source_item_id = f'{source}_item_id'\n",
    "target_item_id = f'{target}_item_id'\n",
    "\n",
    "pair_id_col = f'{source}_{target}_id'\n",
    "\n",
    "# Create pair id\n",
    "df_match[pair_id_col] = df_match[[source_item_id, target_item_id]].apply(tuple, axis=1).str.join('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9878f1",
   "metadata": {},
   "source": [
    "### Process Matching 2nd (String Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324e5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_name = 0.7\n",
    "score_name_col = f'{source}_{target}_score_name'\n",
    "flag_name_col = f'{source}_{target}_flag_name'\n",
    "source_name_col = f'{source}_clean_name'\n",
    "target_name_col = f'{target}_clean_name'\n",
    "\n",
    "# Scoring name pair\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df_match[score_name_col] = df_match.progress_apply(\n",
    "    lambda row: fuzz.token_set_ratio(\n",
    "        row[source_name_col], \n",
    "        row[target_name_col]\n",
    "    ) / 100,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Create flag name\n",
    "df_match[flag_name_col] = (df_match[score_name_col] >= threshold_name).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f5a218",
   "metadata": {},
   "source": [
    "### Process Matching 2nd (Embeddings Name) - BACKUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b7a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKUP\n",
    "# # Import embeddings name\n",
    "# json_embed = data_import_json(\n",
    "#     website=website,\n",
    "#     content_date=content_date,\n",
    "#     version=version,\n",
    "#     folder_name=f'normalize/{website}',\n",
    "#     additional_info='embeddings_name'\n",
    "# )\n",
    "\n",
    "# # Convert to pandas\n",
    "# df_embed = pd.DataFrame(json_embed['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db65e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge back to df_match\n",
    "# source_embed_col = f'{source}_embed_name'\n",
    "# target_embed_col = f'{target}_embed_name'\n",
    "\n",
    "# source_key_id = f'{source}_key_id'\n",
    "# target_key_id = f'{target}_key_id'\n",
    "\n",
    "# df_match[source_embed_col] = df_match[source_key_id].map(\n",
    "#     df_embed.set_index('key_id')['embeddings_name'].to_dict()\n",
    "# )\n",
    "\n",
    "# df_match[target_embed_col] = df_match[target_key_id].map(\n",
    "#     df_embed.set_index('key_id')['embeddings_name'].to_dict()\n",
    "# )\n",
    "\n",
    "# # Scoring\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Custom cosine similarity function\n",
    "# def cosine_score(pair):\n",
    "#     x, y = pair\n",
    "#     return cosine_similarity([x], [y])[0][0]\n",
    "\n",
    "# def manhattan_score(pair):\n",
    "#     x, y = pair\n",
    "#     x = np.array(x)\n",
    "#     y = np.array(y)\n",
    "#     return 1 / (1 + np.sum(np.abs(x - y)))\n",
    "\n",
    "# from numpy.linalg import norm\n",
    "# def euclidean_score(pair):\n",
    "#     x, y = pair\n",
    "#     x = np.array(x)\n",
    "#     y = np.array(y)\n",
    "#     return 1 / (1 + norm(x - y))  # scale to (0, 1)\n",
    "\n",
    "# df_match[score_name_col] = df_match[[source_embed_col, target_embed_col]].apply(tuple, axis=1).progress_apply(cosine_score)\n",
    "\n",
    "# df_match.drop(columns=[source_embed_col, target_embed_col], inplace=True)\n",
    "\n",
    "# # Create flag name\n",
    "# df_match[flag_name_col] = (df_match[score_name_col] >= threshold_name).astype(int)\n",
    "\n",
    "# # Create pair id\n",
    "# df_match[pair_id_col] = df_match[[source_item_id, target_item_id]].apply(tuple, axis=1).str.join('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4917d",
   "metadata": {},
   "source": [
    "### Process Matching 3rd (Price Diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d6d308",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_price = 0.3 # percentage\n",
    "score_price_col = f'{source}_{target}_score_price'\n",
    "flag_price_col = f'{source}_{target}_flag_price'\n",
    "source_price_col = f'{source}_clean_price'\n",
    "target_price_col = f'{target}_clean_price'\n",
    "\n",
    "flag_final_col = 'result'\n",
    "\n",
    "# Scoring price diff pair in percentage\n",
    "df_match[score_price_col] = abs(df_match[source_price_col] - df_match[target_price_col])/df_match[source_price_col]\n",
    "\n",
    "# Create flag price\n",
    "df_match[flag_price_col] = (df_match[score_price_col] <= threshold_price).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a19fc00",
   "metadata": {},
   "source": [
    "### Result Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf1dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_brand_col = f'{source}_{target}_rank_brand'\n",
    "rank_name_col = f'{source}_{target}_rank_name'\n",
    "rank_price_col = f'{source}_{target}_rank_price'\n",
    "\n",
    "# Filter Necessary columns only\n",
    "leading_cols = [f'{source}_{target}_id']\n",
    "dynamic_cols = ['scrape_date', 'item_id', 'item_url', 'brand', 'item_name']\n",
    "dynamic_cols = [f'{prefix}_{col}' for col in dynamic_cols for prefix in [source, target]]\n",
    "trailing_cols = [score_brand_col, flag_brand_col, score_name_col, flag_name_col, score_price_col, flag_price_col]\n",
    "\n",
    "df_match = df_match[leading_cols + dynamic_cols + trailing_cols].copy(deep=True)\n",
    "\n",
    "# Filter only matched brand and matched name\n",
    "df_match_candidate = df_match[\n",
    "    (df_match[flag_brand_col] == 1)\n",
    "    &\n",
    "    (df_match[flag_name_col] == 1)\n",
    "    &\n",
    "    (df_match[flag_price_col] == 1)\n",
    "].copy(deep=True)\n",
    "\n",
    "df_match_candidate.sort_values(by=[score_brand_col, score_name_col], ascending=[False, False], inplace=True)\n",
    "\n",
    "# Rank brand to handle multiple matches\n",
    "df_match_candidate[rank_brand_col] = df_match_candidate.groupby(source_item_id)[score_brand_col].rank(\n",
    "    method='dense',\n",
    "    ascending=False\n",
    ").astype(int)\n",
    "\n",
    "# Rank name to handle multiple matches\n",
    "df_match_candidate[rank_name_col] = df_match_candidate.groupby(source_item_id)[score_name_col].rank(\n",
    "    method='dense',\n",
    "    ascending=False\n",
    ").astype(int)\n",
    "\n",
    "# Rank name to handle multiple matches\n",
    "df_match_candidate[rank_price_col] = df_match_candidate.groupby(source_item_id)[score_price_col].rank(\n",
    "    method='dense',\n",
    "    ascending=False\n",
    ").astype(int)\n",
    "\n",
    "df_match_candidate.sort_values(by=[rank_name_col, rank_price_col], ascending=[True, True], inplace=True)\n",
    "\n",
    "# Final decision match/not match\n",
    "df_match_candidate[flag_final_col] = np.where(\n",
    "    (df_match_candidate[rank_name_col] == 1)\n",
    "    &\n",
    "    (~df_match_candidate[source_item_id].duplicated()),\n",
    "    'match',\n",
    "    'not match'\n",
    ")\n",
    "\n",
    "data_export_pandas(\n",
    "    df_output=df_match_candidate,\n",
    "    website=f'{source}_{target}',\n",
    "    content_date=content_date,\n",
    "    version=version,\n",
    "    folder_name=f'matching/{website}',\n",
    "    additional_info='matching_candidate',\n",
    "    incl_excel=True\n",
    ")\n",
    "\n",
    "# Filter only for matched final result\n",
    "df_match_matched = df_match_candidate[df_match_candidate[flag_final_col] == 'match'].copy(deep=True).reset_index(drop=True)\n",
    "\n",
    "data_export_pandas(\n",
    "    df_output=df_match_matched,\n",
    "    website=f'{source}_{target}',\n",
    "    content_date=content_date,\n",
    "    version=version,\n",
    "    folder_name=f'matching/{website}',\n",
    "    additional_info='matching_matched',\n",
    "    incl_excel=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge back result to the complete pair\n",
    "df_match[flag_final_col] = np.where(\n",
    "    df_match[pair_id_col].isin(\n",
    "        df_match_matched[pair_id_col]\n",
    "    ),\n",
    "    'match',\n",
    "    'not match'\n",
    ")\n",
    "\n",
    "data_export_pandas(\n",
    "    df_output=df_match,\n",
    "    website=f'{source}_{target}',\n",
    "    content_date=content_date,\n",
    "    version=version,\n",
    "    folder_name=f'matching/{website}',\n",
    "    additional_info='matching_complete_pair'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16508669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_match_matched[source_item_id].unique())/len(df_source)\n",
    "# len(df_match_matched[target_item_id].unique())/len(df_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab2e96",
   "metadata": {},
   "source": [
    "### Output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
