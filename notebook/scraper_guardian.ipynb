{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acc635b6",
   "metadata": {},
   "source": [
    "# Guardian Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2655692",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c7c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Automatically detect the repo root (parent of notebook folder)\n",
    "repo_root = Path().resolve().parent  # if notebook is in 'notebooks/' folder\n",
    "sys.path.append(str(repo_root))\n",
    "\n",
    "from config.config import get_environment\n",
    "\n",
    "from config.config import data_import_json, data_export_json, data_import_pandas, data_export_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db4dc4",
   "metadata": {},
   "source": [
    "## ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b401cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = get_environment(\n",
    "    env_path=\"../environments\",\n",
    "    env_name=\"env.json\"\n",
    ")\n",
    "\n",
    "# content_date = datetime.now().date() + timedelta(days=0)\n",
    "content_date = ENV['CONTENT_DATE']\n",
    "website = ENV['TARGET'][\"1\"]['NAME']\n",
    "version = ENV['VERSION']\n",
    "\n",
    "url_scrape = ENV['TARGET'][\"1\"]['URL']\n",
    "\n",
    "reparse_only = ENV['SCRAPER']['REPARSE_ONLY']\n",
    "continue_scraper = ENV['SCRAPER']['CONTINUE_SCRAPER']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60687898",
   "metadata": {},
   "source": [
    "## Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ef53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate looping count\n",
    "range_limit = 100 # default maximum limit per page\n",
    "\n",
    "headers = {\n",
    "    'accept': '*/*',\n",
    "    'accept-language': 'en-US,en;q=0.9,id;q=0.8',\n",
    "    'authorization': '',\n",
    "    'content-type': 'application/json',\n",
    "    'newrelic': 'eyJ2IjpbMCwxXSwiZCI6eyJ0eSI6IkJyb3dzZXIiLCJhYyI6IjMyOTYzMjAiLCJhcCI6IjExMTk5OTg1NDciLCJpZCI6ImQ0Y2M2Nzk1M2RmMjc3MzUiLCJ0ciI6IjA3NmE1MDBiNjYwNzNkMDUzMGZhNTEwZjM2YmYxN2E2IiwidGkiOjE3NjQxNDkzODY3NzcsInRrIjoiMTMyMjg0MCJ9fQ==',\n",
    "    'priority': 'u=1, i',\n",
    "    'referer': 'https://www.guardianindonesia.co.id/skincare.html?page=1',\n",
    "    'sec-ch-ua': '\"Chromium\";v=\"142\", \"Google Chrome\";v=\"142\", \"Not_A Brand\";v=\"99\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "    'sec-fetch-dest': 'empty',\n",
    "    'sec-fetch-mode': 'cors',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'store': 'default',\n",
    "    'traceparent': '00-076a500b66073d0530fa510f36bf17a6-d4cc67953df27735-01',\n",
    "    'tracestate': '1322840@nr=0-1-3296320-1119998547-d4cc67953df27735----1764149386777',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36',\n",
    "    'x-magento-cache-id': '2c3b02c55fa16a088ac257d464cf3c6604a62bf574d62984f52d6fda5f18ffdf',\n",
    "    # 'cookie': 'private_content_version=69a83c7730183ab62c52e2c9b77d36dc; _gcl_au=1.1.487326357.1764085693; _tt_enable_cookie=1; _ttp=01KAXV6YEKK6S547A0KDCNHW3G_.tt.2; _gid=GA1.3.1844534993.1764085695; _fbp=fb.2.1764085698203.780253945897702889; _dc_gtm_UA-143423482-1=1; _gat_UA-143423482-1=1; ttcsid=1764149127881::odxZxED0C69bgue3KtGJ.2.1764149386381.0; ttcsid_CTHA7FJC77UD328UMEL0=1764149127880::eUyh6q8NcBBiW7vk3DDm.2.1764149386381.0; _ga=GA1.1.275656929.1764085694; _ga_6KEZ58MRZ7=GS2.1.s1764148446$o2$g1$t1764149386$j58$l0$h0',\n",
    "}\n",
    "\n",
    "total_pages = 0\n",
    "loop_count = 0\n",
    "url_scrape = 'https://www.guardianindonesia.co.id/graphql'\n",
    "while loop_count < total_pages or loop_count == 0:\n",
    "    page = loop_count+1\n",
    "    params = {\n",
    "        # 'query': 'query GetCategories($id:String!$pageSize:Int!$currentPage:Int!$filters:ProductAttributeFilterInput!$sort:ProductAttributeSortInput){categories(filters:{category_uid:{in:[$id]}}){items{uid ...CategoryFragment __typename}__typename}products(pageSize:$pageSize currentPage:$currentPage filter:$filters sort:$sort){...ProductsFragment __typename}}fragment CategoryFragment on CategoryTree{id uid meta_title meta_keywords meta_description image __typename}fragment ProductsFragment on Products{items{id uid categories{id uid name level __typename}name price_range{minimum_price{discount{amount_off percent_off __typename}final_price{currency value __typename}regular_price{currency value __typename}__typename}maximum_price{regular_price{currency value __typename}final_price{currency value __typename}discount{amount_off percent_off __typename}__typename}__typename}sku special_price price{regularPrice{amount{currency value __typename}__typename}__typename}small_image{url __typename}stock_status stock_item{qty min_qty min_sale_qty max_sale_qty backorders qty_increments manage_stock is_in_stock __typename}type_id url_key url_suffix sale promo __typename}page_info{total_pages current_page page_size __typename}total_count __typename}',\n",
    "        'query': 'query GetCategoriesFull($id:String!$pageSize:Int!$currentPage:Int!$filters:ProductAttributeFilterInput!$sort:ProductAttributeSortInput){categories(filters:{category_uid:{in:[$id]}}){items{uid ...CategoryFragment __typename children{uid id name meta_title meta_keywords meta_description image __typename}}__typename}products(pageSize:$pageSize currentPage:$currentPage filter:$filters sort:$sort){items{id uid type_id url_key name sku sale promo special_price stock_status small_image{url __typename}price{regularPrice{amount{currency value __typename}__typename}__typename}price_range{minimum_price{discount{amount_off percent_off __typename}final_price{currency value __typename}regular_price{currency value __typename}__typename}maximum_price{discount{amount_off percent_off __typename}final_price{currency value __typename}regular_price{currency value __typename}__typename}__typename}stock_item{qty min_qty min_sale_qty max_sale_qty is_in_stock __typename}how_to_use description{html __typename}meta_description more_info{label value __typename}review{reviews_count rating_summary __typename}media_gallery_entries{id label position disabled file __typename}__typename}page_info{total_pages current_page page_size __typename}total_count __typename}}fragment CategoryFragment on CategoryTree{id uid name meta_title meta_keywords meta_description image __typename}',\n",
    "        'operationName': 'GetCategories',\n",
    "        'variables': f'{{\"currentPage\":{page},\"id\":\"MjUz\",\"filters\":{{\"category_uid\":{{\"eq\":\"MjUz\"}}}},\"pageSize\":{range_limit},\"sort\":{{\"position\":\"ASC\",\"stock_status\":\"DESC\"}}}}',\n",
    "    }\n",
    "\n",
    "    print(f\"Get Items Loop Count: {page}/{total_pages} | Range Items: {range_limit} | URL: {url_scrape}\")\n",
    "\n",
    "    response = requests.get(\n",
    "        url_scrape,\n",
    "        params=params,\n",
    "        headers=headers\n",
    "    )\n",
    "\n",
    "    print(f\"Response Items Loop Count: {page}/{total_pages} | Range Items: {range_limit} | URL: {url_scrape} | STATUS CODE: {response.status_code}\")\n",
    "\n",
    "    try:\n",
    "        total_pages = response.json()['data']['products']['page_info']['total_pages']\n",
    "\n",
    "        data_export_json(\n",
    "            data=response.json(),\n",
    "            website=website,\n",
    "            folder_name=f'scraper/{website}',\n",
    "            version=version,\n",
    "            content_date=content_date, # \"0000-00-00\"\n",
    "            additional_info=f\"scrape-page{page}\",\n",
    "            metadata={\n",
    "                \"status_code\": response.status_code,\n",
    "                \"page\": page,\n",
    "                \"range_limit\": range_limit,\n",
    "                \"url_scrape\": url_scrape\n",
    "            }\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        data_export_json(\n",
    "            data=response.text,\n",
    "            website=website,\n",
    "            folder_name=f'scraper/{website}',\n",
    "            version=version,\n",
    "            content_date=content_date, # \"0000-00-00\"\n",
    "            additional_info=f\"scrape-page{page}\",\n",
    "            metadata={\n",
    "                \"status_code\": response.status_code,\n",
    "                \"error\": e,\n",
    "                \"page\": page,\n",
    "                \"range_limit\": range_limit,\n",
    "                \"url_scrape\": url_scrape\n",
    "            }\n",
    "        )\n",
    "\n",
    "    loop_count += 1\n",
    "    if total_pages == 0:\n",
    "        break\n",
    "\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21e968",
   "metadata": {},
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e011403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_item(\n",
    "        response_json: dict,\n",
    "        page: int,\n",
    "        scrape_date: datetime\n",
    "    ):\n",
    "\n",
    "    parsed_list = list()\n",
    "    len_items = len(response_json['data']['products']['items'])\n",
    "    cat_name = response_json['data']['categories']['items'][0]['name']\n",
    "    for index, item in enumerate(response_json['data']['products']['items']):\n",
    "\n",
    "        try:\n",
    "            item_id = item['id']\n",
    "            item_name = item['name']\n",
    "            item_slug = item['url_key']\n",
    "            item_brand = [v for v in item['more_info'] if v['label'] == 'Brand'][0]['value']\n",
    "            stock = item['stock_item']['qty']\n",
    "            item_url = f\"https://guardianindonesia.co.id/{item_slug}.html\"\n",
    "            url_image = item['small_image']['url']\n",
    "            review_total = item['review']['reviews_count']\n",
    "            review_rating = item['review']['rating_summary']\n",
    "\n",
    "            data_dict = dict()\n",
    "            data_dict['scrape_date'] = str(scrape_date)\n",
    "            data_dict['category_slug'] = None\n",
    "            data_dict['category'] = cat_name\n",
    "            data_dict['slug'] = item_slug\n",
    "            data_dict['id'] = item_id\n",
    "            data_dict['brand'] = item_brand\n",
    "            data_dict['name'] = item_name\n",
    "            data_dict['url'] = item_url\n",
    "            data_dict['url_image'] = url_image\n",
    "            data_dict['review_total'] = review_total\n",
    "            data_dict['review_rating'] = review_rating\n",
    "            data_dict['review_recommended'] = None\n",
    "            data_dict['wishlist'] = None\n",
    "            data_dict['variant_id'] = None\n",
    "            data_dict['variant_name'] = None\n",
    "            data_dict['stock'] = stock\n",
    "            data_dict['ean'] = None\n",
    "            data_dict['currency'] = 'IDR'\n",
    "            data_dict['price_range'] = f\"{item['price_range']['minimum_price']['final_price']['value']} - {item['price_range']['maximum_price']['final_price']['value']}\"\n",
    "            data_dict['price'] = item['price_range']['maximum_price']['regular_price']['value']\n",
    "            data_dict['price_after_disc'] = item['price_range']['maximum_price']['final_price']['value']\n",
    "            data_dict['price_disc_type'] = None\n",
    "            data_dict['price_disc'] = item['price_range']['maximum_price']['discount']['amount_off']\n",
    "            data_dict['price_disc_perc'] = item['price_range']['maximum_price']['discount']['percent_off']\n",
    "            data_dict['is_package'] = None\n",
    "\n",
    "        except Exception as e:\n",
    "            data_dict = dict()\n",
    "            print(f\"ERROR Parser Page: {page} | Item Index: {index} | Item Count: {index+1}/{len_items} | URL: {item_url} | {e}\")\n",
    "\n",
    "        parsed_list.append(data_dict)\n",
    "\n",
    "    return parsed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b25d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate looping count\n",
    "range_limit = 100 # default maximum limit per page\n",
    "\n",
    "total_pages = 0\n",
    "loop_count = 0\n",
    "url_scrape = 'https://www.guardianindonesia.co.id/graphql'\n",
    "parsed_list = list()\n",
    "try:\n",
    "    while loop_count < total_pages or loop_count == 0:\n",
    "        page = loop_count+1\n",
    "\n",
    "        print(f\"Parsing Items Loop Count: {page}/{total_pages} | Range Items: {range_limit} | URL: {url_scrape}\")\n",
    "\n",
    "        try:\n",
    "            response_json = data_import_json(\n",
    "                website=website,\n",
    "                folder_name=f'scraper/{website}',\n",
    "                version=version,\n",
    "                content_date=content_date, # \"0000-00-00\"\n",
    "                additional_info=f\"scrape-page{page}\"\n",
    "            )\n",
    "\n",
    "            response_json = response_json['data']\n",
    "\n",
    "            parsed_list_temp = parser_item(\n",
    "                response_json=response_json,\n",
    "                page=page,\n",
    "                scrape_date=content_date\n",
    "            )\n",
    "\n",
    "            total_pages = response_json['data']['products']['page_info']['total_pages']\n",
    "\n",
    "        except Exception as e:\n",
    "            parsed_list_temp = list()\n",
    "            print(e)\n",
    "            break\n",
    "\n",
    "        parsed_list = parsed_list + parsed_list_temp\n",
    "\n",
    "        loop_count += 1\n",
    "        if total_pages == 0:\n",
    "            break\n",
    "\n",
    "\n",
    "    # Convert all list to DataFrame\n",
    "    df_parse = pd.DataFrame(parsed_list)\n",
    "\n",
    "    data_export_pandas(\n",
    "        df_output=df_parse,\n",
    "        website=website,\n",
    "        folder_name=f'parser/{website}',\n",
    "        version=version,\n",
    "        content_date=content_date, # \"0000-00-00\"\n",
    "        additional_info=\"parsed\",\n",
    "        incl_excel=True\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "    df_parse = pd.DataFrame(parsed_list)\n",
    "    data_export_pandas(\n",
    "        df_output=df_parse,\n",
    "        website=website,\n",
    "        folder_name=f'parser/{website}',\n",
    "        version=version,\n",
    "        content_date=content_date, # \"0000-00-00\"\n",
    "        additional_info=\"parsed\",\n",
    "        incl_excel=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
