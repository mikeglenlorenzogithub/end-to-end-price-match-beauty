{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0490837",
   "metadata": {},
   "source": [
    "# Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fa6350",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992bc37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Automatically detect the repo root (parent of notebook folder)\n",
    "repo_root = Path().resolve().parent  # if notebook is in 'notebooks/' folder\n",
    "sys.path.append(str(repo_root))\n",
    "\n",
    "from config.config import get_environment\n",
    "\n",
    "from config.config import data_import_json, data_export_json, data_import_pandas, data_export_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54779f8a",
   "metadata": {},
   "source": [
    "## ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8429e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = get_environment(\n",
    "    env_path=\"../environments\",\n",
    "    env_name=\"env.json\"\n",
    ")\n",
    "\n",
    "# content_date = datetime.now().date() + timedelta(days=0)\n",
    "content_date = ENV['CONTENT_DATE']\n",
    "version = ENV['VERSION']\n",
    "\n",
    "website = 'all'\n",
    "# website = ENV['SOURCE']['NAME']\n",
    "# website = ENV['TARGET'][\"1\"]['NAME']\n",
    "# website = ENV['TARGET'][\"2\"]['NAME']\n",
    "\n",
    "MODEL = ENV['LLM']['GEMINI']['MODEL']\n",
    "API_KEY = ENV['LLM']['GEMINI']['API_KEY']\n",
    "range_input = ENV['LLM']['GEMINI']['RANGE_INPUT']\n",
    "\n",
    "EMB_MODEL = ENV['EMBEDDING']['OPENAI']['MODEL']\n",
    "EMB_API_KEY = ENV['EMBEDDING']['OPENAI']['API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a1af0",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600d8aa6",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c60c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Gemini Dependencies\n",
    "from typing import List, Tuple, Optional, Any\n",
    "from google.genai import types, Client\n",
    "\n",
    "def _build_schema_from_defs(defs: Any) -> types.Schema:\n",
    "    if isinstance(defs, str):\n",
    "        # Primitive type\n",
    "        if defs == \"array_of_strings\" or defs == \"array\":\n",
    "            # Array of simple strings\n",
    "            return types.Schema(type=\"array\", items=types.Schema(type=\"string\"))\n",
    "        return types.Schema(type=defs)\n",
    "\n",
    "    if isinstance(defs, tuple):\n",
    "        if len(defs) == 2:\n",
    "            name, content = defs\n",
    "            # \"v\" tuple just unwraps\n",
    "            if name == \"v\":\n",
    "                return _build_schema_from_defs(content)\n",
    "            return types.Schema(type=\"object\", properties=_build_schema_from_defs(content))\n",
    "        elif len(defs) == 3:\n",
    "            # (name, children, \"array\") -> array of objects\n",
    "            name, children, marker = defs\n",
    "            if marker == \"array\":\n",
    "                return types.Schema(type=\"array\", items=_build_schema_from_defs(children))\n",
    "\n",
    "    if isinstance(defs, list):\n",
    "        props = {}\n",
    "        required = []\n",
    "        for part in defs:\n",
    "            if len(part) == 3 and part[2] == \"array\":\n",
    "                # Array of objects\n",
    "                name, sub, _ = part\n",
    "                props[name] = types.Schema(type=\"array\", items=_build_schema_from_defs(sub))\n",
    "            else:\n",
    "                name, sub = part\n",
    "                if sub == \"array_of_strings\" or sub == \"array\":\n",
    "                    props[name] = types.Schema(type=\"array\", items=types.Schema(type=\"string\"))\n",
    "                else:\n",
    "                    props[name] = _build_schema_from_defs(sub)\n",
    "            required.append(name)\n",
    "        return types.Schema(type=\"object\", properties=props, required=required)\n",
    "\n",
    "    raise ValueError(f\"Invalid schema definition: {defs}\")\n",
    "\n",
    "\n",
    "def _build_types_schema(field_defs: List[Tuple[str, Any]]) -> types.Schema:\n",
    "    \"\"\"\n",
    "    Build the top-level schema as an ARRAY of OBJECTS.\n",
    "\n",
    "    Each top-level tuple (name, sub) becomes a property on the item object.\n",
    "      - If sub is a list -> property = array(items = object defined by sub)\n",
    "      - If sub is a string/tuple -> property = schema returned by _build_schema_from_defs\n",
    "    \"\"\"\n",
    "    item_props = {}\n",
    "    item_required = []\n",
    "\n",
    "    for name, sub in field_defs:\n",
    "        if isinstance(sub, list):\n",
    "            item_schema = _build_schema_from_defs(sub)\n",
    "            item_props[name] = types.Schema(type=\"array\", items=item_schema)\n",
    "        else:\n",
    "            item_props[name] = _build_schema_from_defs(sub)\n",
    "\n",
    "        item_required.append(name)\n",
    "\n",
    "    return types.Schema(\n",
    "        type=\"array\",\n",
    "        items=types.Schema(\n",
    "            type=\"object\",\n",
    "            properties=item_props,\n",
    "            required=item_required\n",
    "        )\n",
    "    )\n",
    "\n",
    "def gemini_process_response(\n",
    "    model_version: str,\n",
    "    api_key: str,\n",
    "    prompt_template: str,\n",
    "    input: str,\n",
    "    column_uid: str,\n",
    "    response_key_list: Optional[List[Tuple[str, str]]] = None,\n",
    "    timeout: int=600\n",
    "):\n",
    "    \"\"\"\n",
    "    Request Gemini model and parse structured list of dicts\n",
    "    \"\"\"\n",
    "    # Ensure uid is always included\n",
    "    if response_key_list is None:\n",
    "        response_key_list = []\n",
    "    response_key_list = [(column_uid, \"string\")] + response_key_list\n",
    "\n",
    "    # Build schema from response_key_list\n",
    "    response_schema = _build_types_schema(response_key_list)\n",
    "\n",
    "    client = Client(api_key=api_key)\n",
    "    for retry in range(3):\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=model_version,\n",
    "                contents=(\n",
    "                    prompt_template +\n",
    "                    \" Output must include uid exactly as provided in the input, without trimming, chopping, or normalization.\"\n",
    "                    \" list input: \" + input\n",
    "                ),\n",
    "                config=types.GenerateContentConfig(\n",
    "                    response_mime_type=\"application/json\",\n",
    "                    response_schema=response_schema,\n",
    "                    thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
    "                    temperature=0,\n",
    "                    top_p=1,\n",
    "                    top_k=1,\n",
    "                ),\n",
    "                # timeout=timeout\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if ('ClientError' in str(e) or '429' in str(e)) and retry < 2:\n",
    "                print(e, 'retry', retry+1)\n",
    "                time.sleep(30)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    if response.candidates[0].finish_reason.name != 'STOP':\n",
    "        raise ValueError(\"response hit token output limit:\", response.usage_metadata.candidates_token_count)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52beb1c",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0fbd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_preprocess_input(\n",
    "        df_input: pd.DataFrame,\n",
    "        content_date: datetime,\n",
    "        version: str\n",
    "    ):\n",
    "\n",
    "    # Assign content_date\n",
    "    df_input['content_date'] = str(content_date)\n",
    "\n",
    "    # Generate UID\n",
    "    df_input['uid'] = df_input.index + 1\n",
    "    df_input['uid'] = df_input['uid'].astype(str)\n",
    "    df_input['version'] = version\n",
    "\n",
    "    return\n",
    "\n",
    "def llm_generate_input(\n",
    "        df_input: pd.DataFrame,\n",
    "        process_column: list\n",
    "    ):\n",
    "\n",
    "    # Convert input to list\n",
    "    input_data = df_input[['uid'] + process_column].apply(dict, axis=1).to_list()\n",
    "\n",
    "    return input_data\n",
    "\n",
    "def llm_request_gemini(\n",
    "        prompt: str,\n",
    "        FIELD_DEFS: list,\n",
    "        input_data: list,\n",
    "        MODEL: str,\n",
    "        API_KEY: str,\n",
    "        timeout: int=600\n",
    "    ):\n",
    "\n",
    "    response = gemini_process_response(\n",
    "        model_version=MODEL,\n",
    "        api_key=API_KEY,\n",
    "        prompt_template=prompt,\n",
    "        input=str(input_data),\n",
    "        column_uid='uid',\n",
    "        response_key_list=FIELD_DEFS,\n",
    "        timeout=timeout\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def llm_dump_response(\n",
    "        response: types.GenerateContentResponse,\n",
    "        content_date: datetime,\n",
    "        website: str,\n",
    "        version: str,\n",
    "        additional_info: str='response'\n",
    "    ):\n",
    "\n",
    "    # Convert to Python dict safely\n",
    "    response_dict = response.model_dump()\n",
    "\n",
    "    # Optionally save to file\n",
    "    data_export_json(\n",
    "        data=response_dict,\n",
    "        website=website,\n",
    "        folder_name=f'llm/{website}',\n",
    "        version=version,\n",
    "        content_date=content_date, # \"0000-00-00\"\n",
    "        additional_info=additional_info\n",
    "    )\n",
    "\n",
    "def llm_merge_response(\n",
    "        df_input: pd.DataFrame,\n",
    "        response: types.GenerateContentResponse,\n",
    "        website: str,\n",
    "        content_date: datetime,\n",
    "        version: str,\n",
    "        folder_name: str,\n",
    "        additional_info: str='cleaning'\n",
    "    ):\n",
    "\n",
    "    # Get token usage\n",
    "    token_input = response.usage_metadata.prompt_token_count\n",
    "    token_output = response.usage_metadata.candidates_token_count\n",
    "    print(f\"{additional_info} | Token Input: {token_input} | Token Output: {token_output}\")\n",
    "\n",
    "    # Convert parsed response to dataframe\n",
    "    df_response = pd.DataFrame(response.parsed)\n",
    "    df_response.insert(0, \"token_input\", token_input)\n",
    "    df_response.insert(1, \"token_output\", token_output)\n",
    "\n",
    "    # # Dump converted response to json\n",
    "    # data_export_pandas(\n",
    "    #     df_output=df_response,\n",
    "    #     website=website,\n",
    "    #     content_date=content_date,\n",
    "    #     version=version,\n",
    "    #     folder_name='llm',\n",
    "    #     additional_info=additional_info, #'response-parsed'\n",
    "    #     # incl_excel=True\n",
    "    # )\n",
    "\n",
    "    # Merge response\n",
    "    df_input_merged = pd.merge(\n",
    "        left=df_input,\n",
    "        right=df_response,\n",
    "        on='uid',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Export Merged Gemini Response with Input Data\n",
    "    data_export_pandas(\n",
    "        df_output=df_input_merged,\n",
    "        website=website,\n",
    "        content_date=content_date,\n",
    "        version=version,\n",
    "        folder_name=folder_name,\n",
    "        additional_info=additional_info\n",
    "    )\n",
    "\n",
    "    return df_input_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea8fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(\n",
    "        df_input: pd.DataFrame,\n",
    "        website: str,\n",
    "        content_date: datetime,\n",
    "        version: str,\n",
    "        prompt: str,\n",
    "        FIELD_DEFS: list,\n",
    "        MODEL: str,\n",
    "        API_KEY: str,\n",
    "        timeout: int=600,\n",
    "        process_column: list=['item_name'],\n",
    "        additional_info: str='extract_dump',\n",
    "    ):\n",
    "\n",
    "    print(\"Preprocessing Input Data\")\n",
    "    try:\n",
    "        llm_preprocess_input(\n",
    "            df_input=df_input,\n",
    "            content_date=content_date,\n",
    "            version=version\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise(e)\n",
    "\n",
    "    print(\"Generating Input List\")\n",
    "    try:\n",
    "        input_data = llm_generate_input(\n",
    "            df_input=df_input,\n",
    "            process_column=process_column\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise(e)\n",
    "\n",
    "    print(\"Request Gemini Response\")\n",
    "    try:\n",
    "        response = llm_request_gemini(\n",
    "            prompt=prompt,\n",
    "            FIELD_DEFS=FIELD_DEFS,\n",
    "            input_data=input_data,\n",
    "            MODEL=MODEL,\n",
    "            API_KEY=API_KEY,\n",
    "            timeout=timeout\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise(e)\n",
    "\n",
    "    print(\"Dump Gemini Response to JSON\")\n",
    "    try:\n",
    "        llm_dump_response(\n",
    "            response=response,\n",
    "            content_date=content_date,\n",
    "            website=website,\n",
    "            version=version,\n",
    "            additional_info=f'response-{additional_info}'\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise(e)\n",
    "\n",
    "    print(\"Merge Gemini Response to Input Data and Export\")\n",
    "    try:\n",
    "        df_input_merge = llm_merge_response(\n",
    "            df_input=df_input,\n",
    "            response=response,\n",
    "            website=website,\n",
    "            content_date=content_date,\n",
    "            version=version,\n",
    "            folder_name=f'normalize/{website}',\n",
    "            additional_info=additional_info\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise(e)\n",
    "\n",
    "    return df_input_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aefa6b",
   "metadata": {},
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64af09c",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e721e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = data_import_pandas(\n",
    "    website=website,\n",
    "    folder_name=f'standardized/{website}',\n",
    "    version=version,\n",
    "    content_date=content_date, # \"0000-00-00\"\n",
    "    additional_info=\"standardized\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate key_id\n",
    "df_input['key_id'] = df_input[['website', 'item_id']].apply(tuple, axis=1).str.join('-')\n",
    "\n",
    "# Fill empty package with 0\n",
    "df_input['is_package'] = df_input['is_package'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a297de8",
   "metadata": {},
   "source": [
    "### Generate Gemini Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f29ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache_input = df_input[\n",
    "#     ['item_id', 'item_name', 'item_variant']\n",
    "# ].rename(columns={\n",
    "#     'item_id': 'uid',\n",
    "#     'item_name': 'i',\n",
    "#     'item_variant': 'o',\n",
    "# }).apply(dict, axis=1).to_list()\n",
    "\n",
    "# cache_input = [str(r) for r in cache_input]\n",
    "\n",
    "# Generate Gemini Cache\n",
    "# def gemini_create_cache(\n",
    "#         model_version: str,\n",
    "#         api_key: str,\n",
    "#         cache_name: str,\n",
    "#         system_instruction: str,\n",
    "#         contents: list[str],\n",
    "#         expiry: str=\"3600s\"\n",
    "#     ):\n",
    "\n",
    "#     \"\"\"\n",
    "#     Generate Gemini Cache\n",
    "#     \"\"\"\n",
    "\n",
    "#     client = Client(api_key=api_key)\n",
    "\n",
    "#     cached = client.caches.create(\n",
    "#         model = model_version,  # or another versioned model supporting caching\n",
    "#         config = types.CreateCachedContentConfig(\n",
    "#             display_name = cache_name, # a name for your cache\n",
    "#             system_instruction = system_instruction, # optional\n",
    "#             contents = contents,\n",
    "#             # optional: specify ttl (time-to-live), e.g. \"3600s\" or a datetime-based expire_time\n",
    "#             ttl = expiry\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     print(\"Created cache:\", cached.name)\n",
    "#     return client, cached\n",
    "\n",
    "# client, cache_variant = gemini_create_cache(\n",
    "#     model_version=MODEL,\n",
    "#     api_key=API_KEY,\n",
    "#     cache_name='Variant Name',\n",
    "#     system_instruction='You are a data cleaning and normalization assistant. Use the context below to answer.',\n",
    "#     contents=cache_input\n",
    "# )\n",
    "\n",
    "# # 2. When you make requests, reference this cache\n",
    "# response = client.models.generate_content(\n",
    "#     model = \"gemini-2.5-flash\",\n",
    "#     contents = \"Now fill the empty item_variant for this item_id and return as uid, i, o\",\n",
    "#     config = types.GenerateContentConfig(\n",
    "#         cached_content = cache_variant.name\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# print(\"GPT:\", response.text)\n",
    "# print(\"Usage metadata:\", response.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d4f219",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1192e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIANT\n",
    "# Generate context_input and subset df to be processed through llm\n",
    "context_input_variant = df_input[df_input['item_variant'].notna()][\n",
    "    ['item_id', 'item_name', 'brand', 'item_variant']\n",
    "].rename(columns={\n",
    "    'item_id': 'uid'\n",
    "}).apply(dict, axis=1).to_list()[:100]\n",
    "\n",
    "context_input_variant = str(context_input_variant)\n",
    "\n",
    "prompt_variant = f\"\"\"\n",
    "You are a data cleaning and normalization assistant.\n",
    "\n",
    "Your task is to take a messy text containing multiple information and convert it into a clean, structured JSON object. \n",
    "Make sure to:\n",
    "- Extract the `item_variant` from the `item_name` of each product.\n",
    "- Return empty result with empty string if you couldn't figure out the answer.\n",
    "- Do not return irrelevant text or optional output.\n",
    "\n",
    "Return output **strictly** in JSON with these fields:\n",
    "`uid`, `v` as `item_variant`.\n",
    "\n",
    "Example:\n",
    "\n",
    "{context_input_variant}\n",
    "\n",
    "Now extract variant of the following input:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "FIELD_DEFS_variant = [\n",
    "    (\"r\", [\n",
    "        (\"v\", \"string\")\n",
    "    ]),\n",
    "]\n",
    "\n",
    "process_column_variant = ['item_name']\n",
    "result_column_variant = ['item_variant']\n",
    "\n",
    "\n",
    "# NAME\n",
    "# Generate context_input and subset df to be processed through llm\n",
    "context_input_name = df_input[df_input['website'] == 'sociolla'][\n",
    "    ['item_id', 'item_name', 'brand', 'item_variant']\n",
    "].rename(columns={\n",
    "    'item_id': 'uid'\n",
    "}).apply(dict, axis=1).to_list()[:100]\n",
    "\n",
    "context_input_name = str(context_input_name)\n",
    "\n",
    "prompt_name = f\"\"\"\n",
    "You are a data cleaning and normalization assistant.\n",
    "\n",
    "Your task is to take a messy text containing multiple information and convert it into a clean, structured JSON object. \n",
    "Make sure to:\n",
    "- Extract end of `brand` index string from `item_name` properly even though the brand is in variation, lowercase uppercase version, or abbreviation.\n",
    "- For example {{'brand': 'L'OREAL', 'item_name': 'L'oreal Ultra Care Body Lotion 621ml + Pouch'}} `brand` index string '7' since it is the end of brand index in item_name including symbols and whitespaces between the brand of item_name.\n",
    "- Remember to retrieve index from the item_name, not from the separated brand.\n",
    "- Return `0` if the `brand` does not exist in `item_name`.\n",
    "- Do not return irrelevant text or optional output.\n",
    "\n",
    "Return output **strictly** in JSON with these fields:\n",
    "`uid`, `i` as `brand index string`.\n",
    "\n",
    "Example context:\n",
    "\n",
    "{context_input_name}\n",
    "\n",
    "Now extract index of the following input:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "FIELD_DEFS_name = [\n",
    "    (\"r\", [\n",
    "        (\"i\", \"string\")\n",
    "    ]),\n",
    "]\n",
    "\n",
    "process_column_name = ['item_name']\n",
    "result_column_name = ['index_brand']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83add70d",
   "metadata": {},
   "source": [
    "### Execute LLM Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3e992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_llm_variant(\n",
    "        df_process: pd.DataFrame\n",
    "    ):\n",
    "\n",
    "    df_process['item_variant'] = df_process['r'].str[0].str['v']\n",
    "\n",
    "    return\n",
    "\n",
    "def response_llm_name(\n",
    "        df_process: pd.DataFrame\n",
    "    ):\n",
    "\n",
    "    df_process['brand_index'] = df_process['r'].str[0].str['i']\n",
    "    df_process['brand_index'] = df_process['brand_index'].astype(int)\n",
    "    df_process['item_name'] = df_process[['item_name', 'brand_index']].apply(dict, axis=1).apply(lambda v: v['item_name'][v['brand_index']:])\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1af4f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_value(\n",
    "        df_input: pd.DataFrame,\n",
    "        df_process: pd.DataFrame,\n",
    "        revert_cols: list=['item_name', 'item_variant']\n",
    "    ):\n",
    "\n",
    "    for col in revert_cols:\n",
    "        df_input[col] = np.where(\n",
    "            df_input['key_id'].isin(\n",
    "                df_process[\n",
    "                    (df_process[col] != '')\n",
    "                    &\n",
    "                    df_process[col].notna()\n",
    "                ]['key_id'].unique()\n",
    "            ),\n",
    "            df_input['key_id'].map(\n",
    "                df_process[['key_id'] + [col]].set_index('key_id')[col].to_dict()\n",
    "            ),\n",
    "            df_input[col]\n",
    "        )\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5cd853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_value(\n",
    "        df_input: pd.DataFrame\n",
    "    ):\n",
    "\n",
    "    df_input['clean_brand'] = df_input['brand'].str.lower()\n",
    "    df_input['clean_name'] = df_input['item_name'].str.lower().str.replace(r'(\\d+)', r' \\1 ', regex=True).str.split().str.join(' ')\n",
    "    df_input['clean_variant'] = np.where(\n",
    "        (df_input['item_variant'] != '') & df_input['item_variant'].notna(),\n",
    "        df_input['item_variant'].str.lower(),\n",
    "        'no info'\n",
    "    )\n",
    "    df_input['clean_price'] = df_input['price']\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6e85e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if website in ['sociolla']:\n",
    "#     prompt = prompt_sociolla\n",
    "#     FIELD_DEFS = FIELD_DEFS_sociolla\n",
    "#     process_column = process_column_sociolla\n",
    "#     result_column = result_column_sociolla\n",
    "# elif website in ['guardian']:\n",
    "#     prompt = prompt_guardian\n",
    "#     FIELD_DEFS = FIELD_DEFS_guardian\n",
    "#     process_column = process_column_guardian\n",
    "#     result_column = result_column_guardian\n",
    "# elif website in ['watsons']:\n",
    "#     prompt = prompt_watsons\n",
    "#     FIELD_DEFS = FIELD_DEFS_watsons\n",
    "#     process_column = process_column_watsons\n",
    "#     result_column = result_column_watsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e047184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.config import date_basic\n",
    "# import sys\n",
    "# sys.stdout = open(f\"../data/logging/{date_basic(content_date)}/{date_basic(content_date)}-cleaning.log\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "from math import ceil\n",
    "for info in ['variant']:\n",
    "\n",
    "    if info in ['variant']:\n",
    "        prompt = prompt_variant\n",
    "        FIELD_DEFS = FIELD_DEFS_variant\n",
    "        process_column = process_column_variant\n",
    "        result_column = result_column_variant\n",
    "\n",
    "        # Create temp df for llm process\n",
    "        df_process = df_input[df_input['item_variant'].isna()].copy(deep=True).reset_index(drop=True)\n",
    "\n",
    "    elif info in ['name']:\n",
    "        prompt = prompt_name\n",
    "        FIELD_DEFS = FIELD_DEFS_name\n",
    "        process_column = process_column_name\n",
    "        result_column = result_column_name\n",
    "\n",
    "        # Create temp df for llm process\n",
    "        df_process = df_input[df_input['website'] != 'sociolla'].copy(deep=True).reset_index(drop=True)\n",
    "\n",
    "    len_input = len(df_process)\n",
    "    loop_count = ceil(len_input/range_input)\n",
    "\n",
    "    borders = [(loop*range_input, (loop+1)*range_input) for loop in range(loop_count)][:-1]\n",
    "    borders = borders + [((loop_count-1)*range_input, len_input)]\n",
    "\n",
    "    # df_process_result = pd.DataFrame()\n",
    "\n",
    "    for lower_border, upper_border in borders:\n",
    "\n",
    "        if lower_border < 1750:\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing: {lower_border} - {upper_border}\")\n",
    "        df_process_temp = df_process.iloc[lower_border:upper_border].copy(deep=True)\n",
    "\n",
    "        df_process_temp = extract(\n",
    "            df_input=df_process_temp,\n",
    "            process_column=process_column,\n",
    "            website=website,\n",
    "            content_date=content_date,\n",
    "            version=version,\n",
    "            prompt=prompt,\n",
    "            FIELD_DEFS=FIELD_DEFS,\n",
    "            MODEL=MODEL,\n",
    "            API_KEY=API_KEY,\n",
    "            timeout=600, # in s\n",
    "            additional_info=f'extract_dump_{info}_{lower_border}_{upper_border}'\n",
    "        )\n",
    "        print(f\"Completed: {lower_border} - {upper_border}\")\n",
    "\n",
    "        df_process_result = pd.concat([\n",
    "            df_process_result,\n",
    "            df_process_temp\n",
    "        ])\n",
    "\n",
    "    df_process_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Fill column value with llm response value\n",
    "    if info in ['variant']:\n",
    "        response_llm_variant(\n",
    "            df_process=df_process_result\n",
    "        )\n",
    "\n",
    "    elif info in ['name']:\n",
    "        response_llm_name(\n",
    "            df_process=df_process_result\n",
    "        )\n",
    "\n",
    "    # Revert llm response to the df_input\n",
    "    revert_value(\n",
    "        df_input=df_input,\n",
    "        df_process=df_process_result,\n",
    "        revert_cols=result_column\n",
    "    )\n",
    "\n",
    "# Lowercase and fill empty value with no info and split numeric name\n",
    "normalize_value(\n",
    "    df_input=df_input\n",
    ")\n",
    "\n",
    "data_export_pandas(\n",
    "    df_output=df_input,\n",
    "    website=website,\n",
    "    content_date=content_date,\n",
    "    version=version,\n",
    "    folder_name=f'normalize/{website}',\n",
    "    additional_info='normalize',\n",
    "    incl_excel=True\n",
    ")\n",
    "\n",
    "# sys.stdout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e44c860",
   "metadata": {},
   "source": [
    "## Embeddings Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a7afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# 3b. Embeddings for qualification (batched)\n",
    "\n",
    "def get_embeddings_batch(\n",
    "        input_list: list,\n",
    "        api_key: str,\n",
    "        model: str=\"text-embedding-3-small\",\n",
    "        range_input: int=50,\n",
    "        sleep_sec: float=0.5\n",
    "    ):\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(input_list), range_input), desc=\"Embedding\"):\n",
    "        batch_texts = input_list[i:i+range_input]\n",
    "\n",
    "        retry = 1\n",
    "        while True:\n",
    "            try:            \n",
    "                response = client.embeddings.create(\n",
    "                    model=model,\n",
    "                    input=batch_texts\n",
    "                )\n",
    "\n",
    "                # response.usage.prompt_tokens\n",
    "                # response.usage.total_tokens\n",
    "\n",
    "                batch_embeddings = [item.embedding for item in response.data]\n",
    "                embeddings.extend(batch_embeddings)\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                if retry > 3:\n",
    "                    raise e\n",
    "                else:\n",
    "                    retry += 1\n",
    "                    print(e)\n",
    "\n",
    "            time.sleep(sleep_sec)\n",
    "    return embeddings\n",
    "\n",
    "X_embed = get_embeddings_batch(\n",
    "    input_list=df_input['clean_name'].tolist(),\n",
    "    api_key=EMB_API_KEY,\n",
    "    model=EMB_MODEL\n",
    ")\n",
    "\n",
    "df_input['embeddings_name'] = [list(vec) for vec in X_embed]\n",
    "\n",
    "data_export_json(\n",
    "    data=df_input[['key_id', 'embeddings_name']].apply(dict, axis=1).to_list(),\n",
    "    website=website,\n",
    "    content_date=content_date,\n",
    "    version=version,\n",
    "    folder_name=f'normalize/{website}',\n",
    "    additional_info='embeddings_name',\n",
    "    metadata={\n",
    "        'source_embeddings': 'clean_name'\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
